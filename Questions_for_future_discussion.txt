-> How does the Byte-pair encoding algorithm work? (in depth)

-> absolute vs relative, which positional embedding should we use? (gpt models use absolute, this embedding is also optimized during training while original transformer has fixed positional embedding!)