The best performing models are often blends of several models (also called ensembles) that cannot be interpreted, even if each single model could be interpreted. If you focus only on performance, you will automatically get more and more opaque models. The winning models on machine learning competitions are often ensembles of models or very complex models such as boosted trees(gradient) or deep neural networks.


 The interpretability of the features is a big assumption. But if it is hard to understand the input features, it is even harder to understand what the model does


explainability = interpretability + additional context


it is default for a machine learning model to pick up bias from training data. but thankfully, 
interpretability can be a great tool to detect biasness of a model.

(proxy feature is gameable) vs (causal feature that are not gameable)

google flu outbreak prediction. 


We tend to think in counterfactual cases


Humans don’t want a complete explanation for a prediction, {counterfactual explanations}
but want to compare what the differences were to another instance’s prediction (can be an artificial one)

 We are used to selecting one or two causes from a variety of possible causes as THE explanation. As proof, turn on the TV news


Pay attention to the social environment of your machine learning application and the target audience. Getting the social part of the machine learning model right depends entirely on your specific application. Find experts from the humanities (e.g., psychologists and sociologists) to help you.

If one of the input features for a prediction was abnormal in any sense (like a rare category of a categorical feature) and the feature influenced the prediction, it should be included in an explanation, even if other ‘normal’ features have the same influence on the prediction as the abnormal one

Good explanations prove to be true in reality (i.e., in other situations). But disturbingly, this is not the most important factor for a “good” explanation. For example, selectiveness seems to be more important than truthfulness. The explanation should predict the event as truthfully as possible, which in machine learning is sometimes called fidelity. So if we say that a second balcony increases the price of a house, then that also should apply to other houses (or at least to similar houses). For humans, fidelity of an explanation is not as important as its selectivity, its contrast, and its social aspect.


Humans tend to ignore information that is inconsistent with their prior beliefs. This effect is called confirmation bias.
People will tend to devalue or ignore explanations that do not agree with their beliefs. Good explanations are consistent with prior beliefs. This is difficult to integrate into machine learning and would probably drastically compromise predictive performance. Our prior belief for the effect of house size on predicted price is that the larger the house, the higher the price. Let’s assume that a model also shows a negative effect of house size on the predicted price for a few houses. The model has learned this because it improves predictive performance (due to some complex interactions), but this behavior strongly contradicts our prior beliefs. You can enforce monotonicity constraints (a feature can only affect the prediction in one direction) or use something like a linear model that has this property.


Abnormal causes are by definition rare in the given scenario. In the absence of an abnormal event, a general explanation is considered a good explanation. Generality can easily be measured by the feature’s support, which is the number of instances to which the explanation applies divided by the total number of instances.

Goals of interpretability
--------------------------
-> improving model
-> justifying model and prediction
-> discovering insights


note: models that have learned to take “shortcuts,” like relying on non-causal features to make predictions. The tricky thing about these shortcuts is that they often don’t decrease model performance – they might actually increase it. Like relying on snow in the background to classify whether a picture shows a wolf or a dog


Methods overview
----------------
------------------


Inherently interpretable models:

linear regression, logistic regression, decision tree, linear model extensions, rule fit, decision rules

here rule fit means to Combine tree-based rules with Lasso regression to learn sparse rule-based models

what is lasso regression?


other interpretable-by-design approaches:
-> Prototype-based neural networks for image classification, called ProtoViT
-> Yang et al. (2024) proposed inherently interpretable tree ensembles which are boosted trees (e.g., with XGBoost) with adjusted hyperparameters
-> Model-based boosting is an additive modeling framework. The trained model is a weighted sum of linear effects, splines, tree stumps, and other so-called weak learners (Bühlmann and Hothorn 2007).
-> Generalized additive models with automatic interaction detection (Caruana et al. 2015).



Scope of interpretability
--------------------------

->The model is entirely interpretable: like small decision trees

->Parts of the model are interpretable: linear regression with hundreds of features

->The model predictions are interpretable: using KNN for images, showing K most similar images for models particular prediction. Or for decision trees, a prediction is explained by returning the decision list that led to the prediction.

Note: When exploring a new interpretability approach, assess the scope of interpretability. Ask at which levels (entirely interpretable, partially interpretable, or interpretable predictions) the approach operates.

# Many data-driven fields already have established (interpretable) modeling approaches, such as logistic regression in medical research.

# there are often multiple models with similar performance but different interpretations, which makes our job more difficult. This is called the Rashomon effect.

# extracting data insights vs extracting model insights.

 there are often multiple models with similar performance but different interpretations, which makes our job more difficult. This is called the Rashomon effect. The problem with this model multiplicity is that it makes it very unclear which model to interpret.








