The best performing models are often blends of several models (also called ensembles) that cannot be interpreted, even if each single model could be interpreted. If you focus only on performance, you will automatically get more and more opaque models. The winning models on machine learning competitions are often ensembles of models or very complex models such as boosted trees(gradient) or deep neural networks.


 The interpretability of the features is a big assumption. But if it is hard to understand the input features, it is even harder to understand what the model does


explainability = interpretability + additional context


it is default for a machine learning model to pick up bias from training data. but thankfully, 
interpretability can be a great tool to detect biasness of a model.

(proxy feature is gameable) vs (causal feature that are not gameable)

google flu outbreak prediction. 