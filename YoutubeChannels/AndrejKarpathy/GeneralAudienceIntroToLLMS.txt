video from 2024
Andrej said,

out of several llms out there, meta
open sourced llama270b. they open sourced everything (architecture, parameters and runnable code) (even open AI did not open sourced their architecture)
although openAI chatgpt model, you can utilize it through API call

basically Andrej talked about how inference can be done by just running that .c file which contains the NN architecture its forward passing (No training needed as its parameters are already here)

how parameters are collected?

from the internet and they were trained on GPUs based on metas NN architecture. 

you can call the parameters as basically a zip file (lossy compression) version of the internet. (basically, how this llama was trained)


the hard part is getting the parameters. running the nn on those parameters is fairly computationally very cheap.

what llms do is basically predict the next word and then the predicted next word gets into that same model and predicts next word again.
its like a lossy compression for text distributions which can also be called as "dreams" meaning not everything is memorized and not every thing is hallucinated either!


how do these LLMS work internally?

well, we know the underlying architecture (transformers) and its operations but we don't know billions of parameters work.
also, the knowledge that these models have is kind of weird. models don't provide same response if we leverage its knowledge by asking it from different directions.
Again, all we can do is measure LLMS behavior through asking questions and analyzing answers but definitely don't know exactly how are they providing answers (Interpretability field is currently working on this part but its in its infantry state right now ()).

creating AI assistant by utilizing fine-tuning!
what if we want to turn these big LLMs into personal assistant which would provide answeres to our questions in a very helpful format and manner?
what we can do is create our own dataset by hiring bunch of people and tell them to imagine any question and give answers to them but following our own labeling instructions which would be let's say user friendly (exactly what 
big AI companies did to improve their model). After creating this dataset, we can train it to our existing big LLM model which was pretrained having previous knowledge of the internet.

this process above can be called as finetuning which would result in giving us a more organized LLM. why?
because now when you ask questions, it will give you answers in a style that was leveraged through finetuning following knowledges from both this and previous iteration of datasets!  

building the base llms (llms which are not finetuned using QnA dataset) are extremely expensive where as finetuning them is not that costly at all.
even though response of base model is not convenient for user but utilizing the knowledge for future finetuning would result in modern assistants like ChatGPT, Gemini etc!

There is also another stage of finetuning which can be called as utilizing comparison labels.
 
in 3rd stage of fine tuning, we get several answers of same questions from 2nd stage finetuned version and let people compare which output seems better and that better version
gets higher priority in 3rd stage finetuning. this process can be called as reinforcement learning through humnan feedback (rlhf)

the labeling instruction is actually not completely manually taken from human. Infact its a collaboration of human and llms.

recent leaderboard shows topranked language models. these llms are ranked by calculating ELO score.
how its calculated? imagine you ask questions and get answers from lets say 2 llms but you don't know these llms name. and when you select the best answer is how that llm gets better ELO score. on that leaderboard, top models have high elo scores where the open source ones do not possess higher ELO score.

LLM scaling laws suggest that even if you can't improve the algorithms but want to improve llm models performance on various metrics (such as next word predictions etc.)
, you can simply add more gpus and add more data to get that improvement.

based on query, llms can understand which tools (such as calculator, webbrowsing, DALL-E etc) to use! and provide relevant information.
example: imagine for a query gpt provides you a table of numbers in different columns. if you query to plot these inputs, what chatgpt would do is utlize matplot library and literally code
it in python and get the result which it would show as output!

llms can not only generate but also see images and perform tasks based on whats in that image! 
example: imagine a picture having a notebook. Inside that notebook, we have a website designed by pencil.
chatgpt would see this picture and literally code that website using html and java script and you have the website!

modern llms also possess speech to speech communication! another multimodal functionality!


future directions of development in LLMS
-----------------------------------------------------

can we inject system2 work of our brain into LLMS?
--------------------------------------------------
if I say what is the result of 2 + 2 you would quickly give me an anwswer which is 4 instinctively. 
if If I say what is the result of 24*78 then you would think about it, give it time, do some logical operation and then provide answer. (refernce: book called thinking fast and slow)

our brain works in these 2 ways. one, where you provide answers to questions instinctively which we can call system 1 and another where we provide answer by utilizing
a tree of thought and make complex decisions which takes time. another example of system1 and system2 can be respectively speed chess and classical chess (longer time or unlimited time).
currently (in 2024) we have llms that has system 1 capability but lack system 2 option (probably available now in 2025). 
if llms can possess system 2 capability, then querying an llm to "make decision but you can take your time" would do the trick! (it would utilize system 2!)




















