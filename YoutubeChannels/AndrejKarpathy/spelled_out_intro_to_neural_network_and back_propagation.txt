autograd is an automatic gradient engine which implements backpropagation. it is a library created by Andrej karpathy.

backpropagation is an algorithm that efficientlt calculates gradient of some kind of a loss function with respect to weights of a neural network. it would allow
us to tune the weights of neural network which would minimize the loss function and therefore increase the accuracy of the neural network.

backpropagation lies at the heart of pytorch and other modern deep neural network library.


micrograd library made by Andrej karpathy is a tool that we can use to define an expression with certain parameters and get some output.
In behind the scene, micrograd actually does backpropagation!. we can find the gradient or partial differentiation of output with respect to any expression parameter!

basically, you can train a neural network with this! you might wonder what about using tensors of pytorch. tensors just provide parallelism while performing computation and that's
the difference. nothing else. the reason we use tensors is for faster calculation . so, using micrograd would be a huge head-start for understanding fundamentals before heading
to utilizing tensors of py-torch.


micro-grad contains two files. engine.py and nn.py.

engine.py is the file required by nn.py to perform its operation
nn.py contains a function called neuron, layer and mlp (multilayer perceptron)

-> neural network is pretty massive mathematical expressions.
thats why, we need datastructures in order to maintain it. for that we create a class called Value in jupyter notebook.


during Neural Network training, you would be very interesting in finding the derivative(partial) of final output with respect to each and every weights (not data because data is always fixed)

crux of backpropagation: chain rule.
chain rule intuitive explanation: if a car moves 4 times fast than a bicycle and a bicycle moves 8 times faster than a person, then a car would move (4*8) = 32 times faster than a person!

backpropagation is a recursive application of chainrule which goes backwords through the computation graph from final output.

gradient(partial derivative) gives us the power to control the final outcome



Now we will look at backpropagation through a neuron.

each neuron has some input (x1, x2, .... xn) and for each inputs, we have weights or synopsis (w1, w2 .... wn)
inside each neuron
  we calculate (sum of (xi * wi) for all n input) + b) where b is called bias which is another value that has control over sum of (xi*wi) depending on what value is set on that 'b'.

finally, all of these go through a function called activation function that actually represents its input into a specific range. for example tanh (in range -1 to 1), sigmoid etc.


imagine n1 node + n2 node ---> n3 node 
in the above case, n3 nodes gradient value will be distributed as an equal value to node n1 and node n2
meaning n1 grad = n3 grad and n2 grad = n3 grad

addition operation equally distributes the gradients!



