2k25

we are going to dive deep (also considering general audiences) into llms like chat gpt

there are several steps to build llms like gpt

1. pretraining
   ------------
          pretraining is composed of some steps too.
          
          gathering data from the internet.
          ---------------------------------
             to give an idea of what it looks like, hugging face has a dataset called "fine-web" that is 44-Terabyte in size.
             Common Crawl CC(non-profit organization) can be a starting point. what they do is, take some seed websites. then index data from that websites.
             follow every links of those websites and do so on and so forth. The situation can be depicted much like a tree).
              
             but as the common crawl data is very raw, how is it filtered in many different ways?

           URL filtering --> (removing websites that are harmful or malicious, advertisement, adult content etc. )
           text extraction --> take raw data, meaning raw html and extract only texts (for example, a website having a navbar and a paragraph below. crawlers would only be interested in paragraph text)
           language --> different companies set different criteria (example take webpages where we get atleast 65% english)
           ---
           ---
            ---
           PII (personal identification information) ---> try to extract webpages which include private information such as (social security number etc.)



         tokenization:
          the process of converting a sequence of words to symbols (tokens or numbers)
             "I want Spiderman! I want Dr Doom" 
          imagine the above text as a huge sequence of text coming from internet. you see, computer's understand them as a sequence of 0, 1 bit wise way
          if we use an encoding called 'utf8' to that text inside "", lets imagine we get something like below
                           01011101 01010101 0101011 00011111 00010111 00111111
          length of that text sequence is a very important parameter for neural networks. what we want is to trade off between this sequence length and the symbol's that represent that sequence.
           so far, we have 2 symbols for "I want Spiderman!" but as I want to trade off, my target would be to compress that 0,1 sequence length and increase the number of symbols.
              for this, we can form an 8bit group of 256 possible symbols (aka numbers)
             Now that sequence might look like 

                 12 3 45 4 12 3 56 32

            but big llms go even further. they leverage byte-pair algorithm to shrink sequence even more.
            In byte-pair algorithm, we notice  consecutive symbols occurrence and merge them and provide them a new symbol lets give it 256

                 256 45 4 256 56 32

             (Note: gpt 4 has 100,277 symbols but here we are providing examples with only 256 symbols)

           so how does gpt4 converts text to tokens and then from tokens to text?

            (Note: there is a website known as Tiktonizer where you can get a hands on experience about text->tokenizer conversion)
               in Tiktokenizer, you will find that the tokenizer for gpt4 base is known as cl100k base that contains 100,277 possible symbols.
            

        Neural networks are nothing but a big mathematical expression with huge parameters( aka weights) and each token of each context window( let's take size 4. example from above would be 256, 45, 4, 256). training this neural networks with those sequence of tokens (context window lets say 256, 45, 4, 256 --> will predict its next which is 56) after each iteration weights get aligned to a point where it will successfully predict the next token.



model inference:
-------------------
after training part, it's time to talk to the model!
you query a set of text and then model generates text for you.
but what happens underneath?

when you query, you give the model a sequence of tokens which goes to Neural network that would have to pick a token out off 100,277 tokens(gpt 4) using probability
distributions. then that new token generated or inferenced by model is again moved as input to neural network to provide the next token and so on and so forth.


Gpt(generative pre-trained-transformers) -2 by openAI can be good head-start for understanding how modern gpt's work. why?
because what really differs from gpt-2 than current SOTA (state of the art's) is that the architecture is probably same as before.
what increased is in the size of parameters, tokens, context-windows.

so why not make a gpt2 by yourself? then you would get the basic understanding for modern llms workflow!



(model trained by utilizing "Lambda" a company that provides cloud GPU service to the users)
GPU's are perfect fit for training models. but why?
because the deal with expensive computations in a parallel way that would give us faster better output.
example of gpu would be NVIDIA H100  ---> several of these combined can be a node ---> several of node can be combined as a system ---> several of system can be a whole data center system.


"base models" are actually not the question answer type system llms that we see over the internet. All base models can do is just constantly predict next token and give you outputs.
you can call it as one of the very first steps to building large llms like chatGpt, deepseek-R1.


what is regurgitation?
in a base model lets say llama-3, if model provides output exactly the same as it was trained on from a particular webpage.
in short, complete memorization output. not even a slight change.
the reason for regurgitation is when model has seen (epochs) that text many times while training.

what is hallucination?
complete reverse of regurgitation. here model takes its best guess to provide probabilistic output which is completely not legit.

grabing in-context learning ability of llama-3base using few-shot prompt!
imagine you query that base model a pattern of word translation from English to banglish like below
"I:ami, you:tumi, he:"
the model might give the below output
"tini, work:"
here llama-3 is learning the pattern from query that its a series of word translation!


how do you create an AI assistant if you only have the base model?
by making an effective prompt! create a series of dialog conversations like below
human: what is a basket?
assistant: it is an object
---
---
--
human: why the sky is blue?

now llama3-base would provide an answer just like below
assistant: --------------  (let '---' be some random text)

human: ----  (let '---' be some random text)

etc.
but it will also go on and continue the conversation from human perspective which is not exactly what we want but still, it can be some sort of an AI-assistant isn't it?


so base model is an internet document simulator on a token level. it can generate token sequences that have the same statistics as internet documents.

but we need to improve from here as we want actual AI assistant. so now, we move to POST-TRAINING stage.

2. Post-Training   -> here outcome would be an SFT(supervised fine-tuning) model
-----------------
here we turn the base model to an assistant model!
how?
by training the previous base model with a new dataset.
That new dataset is like a conversation between (let's say) an assistant and a human.
computation cost in this phase is much less than pre-training phase (as dataset won't be that big)

but how do we represent this new dataset? how do we make the base model see this new dataset? and how do we create this new dataset?

how do we convert conversations into token space?
see tiktokenizer example (from hugging-face)
basically it follows a set of rules (much like when sending messages using TCP/IP where we universally agree a set of rules on how to send data)


we give conversations a new format.
for example:
instead of training on raw tokens of "what is 2+2?" we train it on raw tokens of "<|im_start|> user <|im_sep|> what is 2+2? <|im_end|>"
   
similarly,
  "<|im_start|> user <|im_sep|> what is 2+2? <|im_end|>
  
  <|im_start|> assistant <|im_sep|> it's 4! like 2*2! <|im_end|>"

see, we added additional rules(additional new tokens) to make model understand while predicting next token(training)

different llms might have different format of making models understand conversation.

so, we are basically finetuning base-llms with conversation data! 

now, when you move to chatgpt, and ask the question "what is 2+2?"
in server, that question will be converted to the  

   ("<|im_start|> user <|im_sep|> what is 2+2? <|im_end|>
   <|im_start|> assistant <|im_sep|>")
 texts token and will be asked the model to provide an output and from that output we show the user every token other than
"<|im_end|>"
   "<|im_start|> user <|im_sep|> what is 2+2? <|im_end|>
   <|im_start|> assistant <|im_sep|>"


how are these conversation data created?
by expert human labelers, combination of llms and expert human labelers (example: ultra chat)

imagine you query something on chatgpt and get some output.
if that question is the same as post-training data, the model would very likely provide the output of that exact(slight change maybe) postraining data 

if that question is not the same as post-training data, the model would gather knowledge from pretraining data and combine it with post-training conversation data (to get the idea of an assistant) and then provide its emerging output!.



LLM Psychology
---------------
what is hallucination?
when llms confidently generate texts that are not legit at all.

how do they generate these hallucination?
"who is john cena?"
let's imagine from the above query, you get the following answer:
"the goat of wwe".
now the response here is legit.

let's imagine, another query
who is zack ryder? (also imagine there was no "zack ryder" name mentioned in training data of this particular llm)

and you get the answer "legend of wwe" 

response of the second query is false. but still, the llm provided the answer confidently!. this phenomenon is known as hallucination.
even though the model does not have any knowledge about "zack ryder" it imitated patterns of the question "who is -----?" and provided its random guess.
Basically, the power of imitating surpasses its uncertainity invocation.


how do we mitigate hallucination?
meta released a paper for llama-3 where they mitigated this phenomenon by
-> querying the same question 3(to5) times to analyze whether the answer differs or not.
   for example:
    who is zack ryder?
   -> overrated superstar of wwe.
   who is zack ryder?
   -> underrated superstar of wwe
   who is zack ryder>
   -> hero of wwe

-> if the answer differs, then we finetune the model with similar to the following conversation
who is zack ryder?
-> "I don't have any knowledge regarding the question"


but there are more advanced ways of dealing with hallucinations too!

by using tools! (web search)
if the model is uncertain about a queries answer, it will have the ability to search on website get the text and add it with the given query, ultimately
feeding this context window (aka working memory) to get an answer from the model that provides reference of website links.

instead of finetuning models with unknown questions answer as "I don't know", we leverage special token designed to query on web to get text from the internet.
during the inference part, if model sensed that it encountered uncertainitiy, it searches that query on web. During this time, it pauses token generation until it gets
information from web and appends it with the actual query (together working as this context window that gets to feed into the neural network) and provides final answer
with reference (probably utilized by RAG).


how do we make the models give authentic information about themselves?
2 ways. 
-> one way would be to create hard coded conversations and finetune them.
-> other way would be to attach hidden system message that is added before any query user provides. Here, system message would contain models information

models need tokens to think!
-------------------------------

for enhancing computational capabilities in LLMS,  the best decision would be to conclude response of query with final answer rather than commencing with final answer.
for example,
you ask llm to give answer of the following equation,
x^2 - 5x + 1 = 0 

it will be a risky situation if model is trained to provide computational answers at the very beginning without going through step by step process.
why?
because neural network has fix number of layers that commit limited operations. if we train computational conversations like the above, then answer token will have to go through
a tough period calculating as it does not have reliability of previous steps.

that's why, we should finetune computational conversations using (divide and conquere) approach, which is to follow step by step proceedure and finally come to conclusion with final answer. but recommended way would be to ask llms to use tool(python interpreter) to code the computation part where llms will generate code compiler would compile the code
and provide answer.

cognitive deficit of models
-> not good at counting (try to make models utilize tools to get better at it)
-> not good at finding characters from a word or sentence. the reason is that models see tokens (as words). they don't see characters. so if we make a character level tokenization, we might solve this issue but it would be a long sequence and might raise further issue (or not, well I'm not sure).

jagged intelligence
-------------------
why is it that a model, able to solve Olympiad level problems, not able to correctly answer the question "which is bigger 9.11 or 9.9?"
it's still a big question. Although one research explained that they were navigating layers and activation functions and found that the above question conflicts with bible page number!
as in bible 9.9 after 9.11 but the above questions actual answer is that 9.11 is bigger than 9.9.

 

3. Reinforcement Learning (last major stage of training)
--------------------------------------------------------
--------------------------------------------------------

big AI companies did not talk publicly about reinforcement-learning-finetuning. then suddenly deepseek-R1 pops and shattered the internet. more about deepseek-r1 later.


the concept is probably still early in its development process.


In the context of LLMS,

step1-> sample the best possible correct answer out of several possible correct answers of a query 
(you run that same query several times to get several answers and sample best answer to train it again with that query).

step2-> train model with that answer for that query

step3-> goto step1


by doing this model will provide the best tokens to get the final answer. it will do it in such a way that is so much favourable to the model itself.



pre-training, post-training and reinforcement-learning-finetuning all together can be described simply if you analyze how 
we complete a particular course subject book (let's say chemistry).
for each chapter this book generally contains,
background knowledge (related to pretraining) (how? when you read them you basically train yourself with that knowledge much like models),
questions and answer from experts (related to post-training) (how? when you review the answers from experts, you basically understand how to write answers in a proper format),
and finally excercise questions (related to reinforcement-learning-finetuning) (how? when you practice by yourself you come up with different solutions that lead to correct and incorrect answers but you pick correct ones and anaylze which solution would be best for you to deliver in exams! and maybe in some extent your solution might get liked by the one who evaluates
your solution!
)















