2k25

we are going to dive deep (also considering general audiences) into llms like chat gpt

there are several steps to build llms like gpt

1. pretraining
   ------------
          pretraining is composed of some steps too.
          
          gathering data from the internet.
          ---------------------------------
             to give an idea of what it looks like, hugging face has a dataset called "fine-web" that is 44-Terabyte in size.
             Common Crawl CC(non-profit organization) can be a starting point. what they do is, take some seed websites. then index data from that websites.
             follow every links of those websites and do so on and so forth. The situation can be depicted much like a tree).
              
             but as the common crawl data is very raw, how is it filtered in many different ways?

           URL filtering --> (removing websites that are harmful or malicious, advertisement, adult content etc. )
           text extraction --> take raw data, meaning raw html and extract only texts (for example, a website having a navbar and a paragraph below. crawlers would only be interested in paragraph text)
           language --> different companies set different criteria (example take webpages where we get atleast 65% english)
           ---
           ---
            ---
           PII (personal identification information) ---> try to extract webpages which include private information such as (social security number etc.)



         tokenization:
          the process of converting a sequence of words to symbols (tokens or numbers)
             "I want Spiderman! I want Dr Doom" 
          imagine the above text as a huge sequence of text coming from internet. you see, computer's understand them as a sequence of 0, 1 bit wise way
          if we use an encoding called 'utf8' to that text inside "", lets imagine we get something like below
                           01011101 01010101 0101011 00011111 00010111 00111111
          length of that text sequence is a very important parameter for neural networks. what we want is to trade off between this sequence length and the symbol's that represent that sequence.
           so far, we have 2 symbols for "I want Spiderman!" but as I want to trade off, my target would be to compress that 0,1 sequence length and increase the number of symbols.
              for this, we can form an 8bit group of 256 possible symbols (aka numbers)
             Now that sequence might look like 

                 12 3 45 4 12 3 56 32

            but big llms go even further. they leverage byte-pair algorithm to shrink sequence even more.
            In byte-pair algorithm, we notice  consecutive symbols occurrence and merge them and provide them a new symbol lets give it 256

                 256 45 4 256 56 32

             (Note: gpt 4 has 100,277 symbols but here we are providing examples with only 256 symbols)

           so how does gpt4 converts text to tokens and then from tokens to text?

            (Note: there is a website known as Tiktonizer where you can get a hands on experience about text->tokenizer conversion)
               in Tiktokenizer, you will find that the tokenizer for gpt4 base is known as cl100k base that contains 100,277 possible symbols.
            

        Neural networks are nothing but a big mathematical expression with huge parameters( aka weights) and each token of each context window( let's take size 4. example from above would be 256, 45, 4, 256). training this neural networks with those sequence of tokens (context window lets say 256, 45, 4, 256 --> will predict its next which is 56) after each iteration weights get aligned to a point where it will successfully predict the next token.



model inference:
-------------------
after training part, it's time to talk to the model!
you query a set of text and then model generates text for you.
but what happens underneath?

when you query, you give the model a sequence of tokens which goes to Neural network that would have to pick a token out off 100,277 tokens(gpt 4) using probability
distributions. then that new token generated or inferenced by model is again moved as input to neural network to provide the next token and so on and so forth.





             
           