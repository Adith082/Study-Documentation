2k25

we are going to dive deep (also considering general audiences) into llms like chat gpt

there are several steps to build llms like gpt

1. pretraining
   ------------
          pretraining is composed of some steps too.
          
          gathering data from the internet.
          ---------------------------------
             to give an idea of what it looks like, hugging face has a dataset called "fine-web" that is 44-Terabyte in size.
             Common Crawl CC(non-profit organization) can be a starting point. what they do is, take some seed websites. then index data from that websites.
             follow every links of those websites and do so on and so forth. The situation can be depicted much like a tree).
              
             but as the common crawl data is very raw, how is it filtered in many different ways?

           URL filtering --> (removing websites that are harmful or malicious, advertisement, adult content etc. )
           text extraction --> take raw data, meaning raw html and extract only texts (for example, a website having a navbar and a paragraph below. crawlers would only be interested in paragraph text)
           language --> different companies set different criteria (example take webpages where we get atleast 65% english)
           ---
           ---
            ---
           PII (personal identification information) ---> try to extract webpages which include private information such as (social security number etc.)



         tokenization:
          the process of converting a sequence of words to symbols (tokens or numbers)
             "I want Spiderman! I want Dr Doom" 
          imagine the above text as a huge sequence of text coming from internet. you see, computer's understand them as a sequence of 0, 1 bit wise way
          if we use an encoding called 'utf8' to that text inside "", lets imagine we get something like below
                           01011101 01010101 0101011 00011111 00010111 00111111
          length of that text sequence is a very important parameter for neural networks. what we want is to trade off between this sequence length and the symbol's that represent that sequence.
           so far, we have 2 symbols for "I want Spiderman!" but as I want to trade off, my target would be to compress that 0,1 sequence length and increase the number of symbols.
              for this, we can form an 8bit group of 256 possible symbols (aka numbers)
             Now that sequence might look like 

                 12 3 45 4 12 3 56 32

            but big llms go even further. they leverage byte-pair algorithm to shrink sequence even more.
            In byte-pair algorithm, we notice  consecutive symbols occurrence and merge them and provide them a new symbol lets give it 256

                 256 45 4 256 56 32

             (Note: gpt 4 has 100,277 symbols but here we are providing examples with only 256 symbols)

           so how does gpt4 converts text to tokens and then from tokens to text?

            (Note: there is a website known as Tiktonizer where you can get a hands on experience about text->tokenizer conversion)
               in Tiktokenizer, you will find that the tokenizer for gpt4 base is known as cl100k base that contains 100,277 possible symbols.
            

        Neural networks are nothing but a big mathematical expression with huge parameters( aka weights) and each token of each context window( let's take size 4. example from above would be 256, 45, 4, 256). training this neural networks with those sequence of tokens (context window lets say 256, 45, 4, 256 --> will predict its next which is 56) after each iteration weights get aligned to a point where it will successfully predict the next token.



model inference:
-------------------
after training part, it's time to talk to the model!
you query a set of text and then model generates text for you.
but what happens underneath?

when you query, you give the model a sequence of tokens which goes to Neural network that would have to pick a token out off 100,277 tokens(gpt 4) using probability
distributions. then that new token generated or inferenced by model is again moved as input to neural network to provide the next token and so on and so forth.


Gpt(generative pre-trained-transformers) -2 by openAI can be good head-start for understanding how modern gpt's work. why?
because what really differs from gpt-2 than current SOTA (state of the art's) is that the architecture is probably same as before.
what increased is in the size of parameters, tokens, context-windows.

so why not make a gpt2 by yourself? then you would get the basic understanding for modern llms workflow!



(model trained by utilizing "Lambda" a company that provides cloud GPU service to the users)
GPU's are perfect fit for training models. but why?
because the deal with expensive computations in a parallel way that would give us faster better output.
example of gpu would be NVIDIA H100  ---> several of these combined can be a node ---> several of node can be combined as a system ---> several of system can be a whole data center system.


"base models" are actually not the question answer type system llms that we see over the internet. All base models can do is just constantly predict next token and give you outputs.
you can call it as one of the very first steps to building large llms like chatGpt, deepseek-R1.


what is regurgitation?
in a base model lets say llama-3, if model provides output exactly the same as it was trained on from a particular webpage.
in short, complete memorization output. not even a slight change.
the reason for regurgitation is when model has seen (epochs) that text many times while training.

what is hallucination?
complete reverse of regurgitation. here model takes its best guess to provide probabilistic output which is completely not legit.

grabing in-context learning ability of llama-3base using few-shot prompt!
imagine you query that base model a pattern of word translation from English to banglish like below
"I:ami, you:tumi, he:"
the model might give the below output
"tini, work:"
here llama-3 is learning the pattern from query that its a series of word translation!


how do you create an AI assistant if you only have the base model?
by making an effective prompt! create a series of dialog conversations like below
human: what is a basket?
assistant: it is an object
---
---
--
human: why the sky is blue?

now llama3-base would provide an answer just like below
assistant: --------------  (let '---' be some random text)

human: ----  (let '---' be some random text)

etc.
but it will also go on and continue the conversation from human perspective which is not exactly what we want but still, it can be some sort of an AI-assistant isn't it?


so base model is an internet document simulator on a token level. it can generate token sequences that have the same statistics as internet documents.

but we need to improve from here as we want actual AI assistant. so now, we move to POST-TRAINING stage.

2. Post-Training
-----------------
here we turn the base model to an assistant model!
how?
by training the previous base model with a new dataset.
That new dataset is like a conversation between (let's say) an assistant and a human.
computation cost in this phase is much less than pre-training phase (as dataset won't be that big)

but how do we represent this new dataset? how do we make the base model see this new dataset? and how do we create this new dataset?














