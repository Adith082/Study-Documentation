-------------> day1 supervised learning -----------------
bag of words are a text representations where we track the frequency of each words

each data sample is a vector of d dimension where d is the number of vocabulary available and for that particular sample 
we have the record of which words  existed and how many times they existed?


imagine my vocabulary has 5 words of the following:
apple, can, bat, I, have (0, 1, 2, 3, 4) th element of vocabulary

if my sample text is "I have have apple"
then the vector representation of the text would look like below
[1, 0, 0, 2, 1] each value refereeing to the frequency of the word in my sample text


----day2-----



dense vs sparse vector representations

in sparse representation, we have many many zeroes. we can efficiently store if we want in sparse representations as we might not care about zeroes

how many ml algorithms can there be?
-> many, could be infinite amount of ml algorithms. example of some of them are given below

  -> decision trees, linear classifiers, artificial neural networks (or deep neural networks), support vector machines etc.

  -> no ml algorithm is best. it all depends on particular problem, particular data which algo would be perfect.

using loss functions, we can figure out which algo would be perfect for a particular data.

loss functions are universally non-negative always. the more moves toward 0 the more better the algorithm fits its data
some loss functions examples are respectively 0/1 loss, squared-loss(we use it for regression(in this type of problem the label is a Real number) problem), absolute-loss

when to use squared loss and when to use absolute-loss? (hint: ridiculous outliers)

most machine learning algorithms are very much capable of memorizing the training dataset!


-> a lot of huge mistakes happened in machine learning by people just because of not being able to split train and test data the right way.



if the data has temporal component, then the best thing would be to split train test based on time. past data in training, future data in testing.
dr. wein berger made a mistake when he splitted spam emails randomly. which even though lead to test data getting 0% error. but in that way train and test data had more same data! (this is called data leakage problem)
because same spam emails can be sent to multiple people. so, in case of temporal data, it is a good idea to split train-test based on time, other wise split uniformly random.

in medical sector. if we get multiple data from a patient, we should not divide that into train and test. each patients data should either be on train or test. Do not divide one patients data into train and test please. that would again create data leakage problem. You train on a set of patient data you test on a different set of patient data.

you might see that we often divide data into train, validation and test data.
but why do we divide to validation data? can't we just deal with only test data.
No because if only deal with test data we might be overfitted with test data. confused?
well, lets say you are conducting experiments based on several models some models might get good accuracy on that particular test data.
But That does not guarantee that that model is generalized. then what to do? you pick the best model based on validation data.
and finally you just see the test set for only once just to see the out come and throw it out. test set is for the ultimate final judgement.
you can tinker your algo based on your validation data or do what ever you want but you will be judged with hidden test data. I hope you get the point.

usually test error would be much less than validation error and validation error would be slightly less than test error.


what is the best split? train-val, 80-20 but it actually really depends on the data.




