-> convolution operation can learn a lot of spatial dimensional data.


computer vision vs machine vision

computer vision vs human vision

there are so many approaches in computer-vision that do not possess deep-learning or machine-learning! they are rule based heuristic!
there are so many approaches in computer-bison that do posses deeplearning, machine learning

early 2010's
people were making filters(rulebased) to change image quality. for example
there is so called Laplacian smoothing filter that can make a noisy image smooth! by just using the filter. 
You can make a filter that could perform transform like that but you have to be a smart person to do that don't you?

similarly we can have laplace edge detector filter(3by3 kernel maybe?)
similarly we have sobel edge detection, canny edge detection (based on how the filters are implemented which give different type of edge only images)

when we use filters, after applying element wise multiplication, we can use some rules like below.
-> always take absolute value.
-> normalize what ever values you get from 0 to 255

how does sobel, canny edge detection works?
when ever we get a sudden change between colors the filters try to detect that edge part!

it is very difficult to come up with one filter that fit all the requirements. 


there are traditional computer vision systems (rulebased aka heuristic system), there are only ml based systems
and most importantly there are (overlap of (computer vision and and ML) --> deeplearning convolution) systems

why is ML based Computervision good?


in 2012 Alexnet came to the picture, a deepCNN trained with imagenet(popular benchmark(if your model performs good in this dataset, 
then you have a very good machine learning mode) dataset) and created history
Alex net architecture introduced maxpooling, relu activation function instead of tanh activation function.
performed by a large margin in imagenet dataset competition.

AlexNet showed that Deep learning can outperform traditional methods (SVM's, random forests etc.)
AlexNet paper showed that using GPU we can train deep neural networks faster!

in machinelearning sometimes we need to manually extract features from data which we feed into algorithm.
but in deep learninign we don't even need to do that. it would automatically extract features and learn patterns to classify!

shallow neural network vs deep neural network

every neural network has 1 input layer and 1 output layer and some hidden layer.
shallow neural network has 1 or 2 hidden layers where deep neural network has so much more hidden layer thats why they are called as 'deep'



lecture 2
-------------
first we are gonna take a look at the traditional computer vision approaches 
(using filters and convolutions which extracts features from the images) before jumping into the deep learning part.
understanding classical filters applying convolution to extract features will give you so much confidence while applying modern computer vision techniques!

you can use these filters as an image preprocessor before moving them into deep learning. if you don't have much computational power, using filters
for image preprocessing can save some deep learning training time!

there are many classical well known filters available called Sobel filter, Laplacian filter, canny filter and Prewitt filter etc.

this filter can also be called as kernel which is a matrix usually 3by3 or 5by5.


if you want the input and output dimension same, then for bigger kernel you require bigger padding, similarly for more strides your require bigger padding.

padding is required if you dont want to shrink the output.

why use filters?
-> for feature extraction: to detect edges, textures, corners etc.

-> noise reduction: smoothing filters like gaussian blur ( a bit blurry image but noise free).

-> enhancement: sharpening edges to make further analysis.

-> pattern recognition: 

how do you create a filter logically?
core idea: approximating derivatives in 2d

derivative of a function f(x) measures how fast a function f(x) is changing on x
here in f(x) can be called as the intensity function of the image
as we dont have continuous function for an image, we use finite differences

but most of the times these images will contain noises, so we need to consider smoothing for that as well as differentiation!

sobel has 2 filters one for detecting edges in the horizontal direction, and if I transpose that filter we get a new filter in the
vertical direction! (how can we deal with noising, check the intuition on shredat panats video cv series vizuara filters and convolutions)


prewit filter: a simplified alternative to sobel filter.
but for detecting edges in complex noisy images sobel is the better choice!

----------------------------------------------------------------------------------------------------------------
Laplacian filters (we do double differentiation) (we can detect any kind of edge using this Laplacian filter!)
----------------------------------------------------------------------------------------------------------------
a filter for edge detection if you some values of it them it will be 0 why? (not the case for noise reduction etc.)

why do some filters look symmetrical?
balanced response, no directional bias and less shifting

let us play with some kernels!

there is a website called setosa.io where you can upload images and play with your custom kernel!

for blurring, sum of kernel values will be 1, for denoising same thing. but for edges it has to be 0

a light ray intuitive example is given to understand different filters (check the video please)

checkout poloclub cnn explainer for cnn visualizer



lecture 3
----------
what is softmax activation fucntion? converting a set of numbers into a probability distribution such that their sum would be equal to 1.
we are bulding a simple neral network not having any convolutional operations and not having any activation functions at all. we are using softmax for final class distribution (even though its
an activation function
)
 as it would be a linear model y = w*x + b, it will not be performing well on image classification. roughly 40-50% of accuracy came out of it.

so what can we do to improve from here? we can use deep neural networks, introduce non-linear activation function functions, we can use convolution operation








